# Part2 Assignment Answers

## 1.

> How well did you choose which hw1 rule set to use for this assignment, and why?  Did you encounter problems using it, and if so, what did you need to adapt?   What lessons did you learn from making the rules work in the application? 

As the submission evolved, the rules that were selected naturally evolved with the approach. The scaling rules from hw1 translated most directly. The full scaling rule chain was implemented: `classify_known_ingredient` matches a recipe ingredient against reference classification facts, falling back to `classify_default_ingredient` ingredients not in the knowledge base. Then `calculate_ingredient_scaling_multiplier` derives a per-ingredient multiplier by combining the target recipe scale factor with the classification-specific scale factor (e.g., baking soda → LEAVENING_AGENT → 70%, vanilla extract → EXTRACT → 60%, salt → SEASONING → 80%, flour → DEFAULT → 100%). Finally, `scale_ingredient_amount` computes the scaled quantity and `convert_scaled_ingredient_to_optimal_measurement_unit` breaks it into human-readable units, respecting heuristic measurements like PINCH and DASH when a recipe is scaled down and amounts become physically infeasible.

Rules associated with ingredient and equipment substitution were not used, as the submission did not advance to the point where they became relevant. We assumed the user had unlimited ingredients, and where equipment constraints were encountered we _attempted_ to apply batching rather than substitution.

The original hw1 batching rules were not used directly because they were too simple. They could identify that batching was required, but they had no understanding of where they were contextually in the overall recipe and we had no rules about what to actually DO once the consequent of `requires_batching` was added to Working Memory. In the end, I _attempted_ to implement batching from scratch using multi-phase dispatch. For example, the transfer dispatch rules plan how many dough balls fit per baking sheet and assert one `pending_transfer` fact per batch, then iteratively allocate sheets and execute transfers. The cook dispatch rules similarly plan how many sheets fit per oven rack, distribute across multiple ovens, and coordinate per-oven preheat substeps.

Several key lessons emerged from making these rules work in a real application:

1. **Context is everything.** The hw1 rules operated on isolated facts, but in an actual planning system every rule needs awareness of the current step, what equipment is in use, what capacity remains, and how many batches have already been allocated. This led to multi-antecedent rules with 4-5 conditions plus NegatedFact guards for iteration control.

2. **Pure rules are not enough for computation.** Complex calculations like how many dough balls fit on a sheet, how many sheets fit in an oven, how to break a scaled measurement into clean units, were implemented as action functions (`action_fn`) attached to rules rather than as additional rule chains. The rule-based dispatch still controls _when_ and _in what order_ these calculations fire, but the actual math lives in Python functions. This is a pragmatic hybrid: the rule engine handles pattern matching, conflict resolution, and chaining, while imperative code handles calculations and data structure manipulation.

3. **Equipment state management required a state machine.** A simple "is equipment available?" check from hw1 became a full lifecycle: AVAILABLE → RESERVED → IN_USE → DIRTY → (cleaning rule) → AVAILABLE. Surface equipment like cooling racks needed special treatment as always-available with infinite capacity. Tracking equipment contents via `equipment_contents` facts was essential for capacity-aware batching.

## 2.

> In addition to your existing system, how would you design a question-asking component which asks for more information regarding the truth of certain assertions in the WM which the systems could not confirm? What heuristics could you use to ensure system efficiency? (guiding the system to ask more selective questions rather than asking users an exhaustive list of questions)

A question-asking component could be a complement to the existing `ExplanationFacility`, operating _during_ inference rather than after it. Where the explanation facility answers "why did the system believe X?", the question-asking component would address "should the system believe X at all?"

**Core Design.** Each `Fact` in Working Memory would gain an optional `confidence` attribute (a float from 0.0 to 1.0). Input facts provided directly by the user like `recipe_ingredient` facts parsed from the recipe file, and reference facts from the Knowledge Base would receive a confidence of 1.0, since these are given as ground truth. Derived facts would inherit confidence from their antecedents: specifically, the minimum confidence across all antecedent facts, optionally reduced by a per-rule reliability weight. For example, if `classify_known_ingredient` matches a recipe ingredient against a known KB classification, the derived `classified_ingredient` fact inherits full confidence. But if the fallback rule `classify_default_ingredient` fires instead (because no reference fact matched), the derived fact would receive a lower confidence to signal that the system is guessing.

When inference completes (or at configurable checkpoints mid-inference), the question-asking component would scan Working Memory for facts whose confidence falls below a configurable threshold. For each such fact, it would use the derivation chain (the same `fact.derivation` dict that the `ExplanationFacility` already walks) to generate a targeted question. For instance: "The system classified CREAM OF TARTAR as DEFAULT (100% scaling). Is this ingredient actually a leavening agent, a stabilizer, or something else?" The user's answer would be asserted as a new reference-level fact, the low-confidence fact would be retracted, and the engine would re-derive from that point forward with the corrected knowledge.

**Heuristics for Selective Questioning.** Naively asking about every uncertain fact would produce an exhausting list of questions. Several heuristics would keep the interaction efficient:

1. **Only question fallback-derived facts.** The scaling engine already has a natural signal for uncertainty: when `classify_default_ingredient` fires instead of `classify_known_ingredient`, that means the Knowledge Base lacked a specific classification. Rather than attaching confidence scores to every fact, the system could simply flag facts derived by designated "fallback" rules. This is the simplest and most targeted heuristic, since fallback rules exist precisely for cases where the KB is incomplete.

2. **Prioritize facts with high downstream impact.** A fact at the root of a long derivation chain affects many downstream conclusions. If `classified_ingredient` is wrong, then `ingredient_scaling_multiplier`, `scaled_ingredient`, and `optimally_scaled_ingredient` are all wrong. The system should walk the derivation graph forward (or maintain a reverse index of "which facts depend on this one") and prioritize questioning facts with the most dependents. Confirming or correcting one high-impact fact eliminates the need to question its entire downstream chain.

3. **Deduplicate by rule pattern.** If the same fallback rule fires for multiple ingredients (e.g., `classify_default_ingredient` fires for both CREAM OF TARTAR and CORN SYRUP), the system could batch these into a single interaction ("The following ingredients were not found in the knowledge base: ...") rather than asking one question per ingredient. This groups questions by the _rule_ that triggered uncertainty rather than by the _fact_ that was produced.

Together these three heuristics reduce questions from "one per fact in WM" to "one per gap in the Knowledge Base," which is exactly the right granularity: the system asks for help only when it genuinely lacks knowledge, and it asks efficiently by grouping related gaps together.

## 3. 

> Discuss the strengths and limitations of the explanation component.  Under what conditions would its explanations be likely to be useful?   Under what conditions would they be likely to be less useful?   How could you change the design to help alleviate these issues?   How could additional knowledge be applied to help, and what knowledge would be needed? 

The best thing about the explanation component is its recursive derivation tree. For any fact in Working Memory, you can ask "why?" and get a complete trace back to the beginning: the rule that derived it, the antecedent facts that triggered that rule, and so on all the way down. Leaf facts are classified as either INPUT (asserted directly from the recipe) or REFERENCE (matched from the Knowledge Base), so you always know what was given versus what was looked up.

This shines in the [Scaling Ingredients](/DOCS/scaling/1.Scaling.md) phase because the derivation chains are linear and nothing gets mutated along the way. `classify_known_ingredient` takes a `recipe_ingredient` and a `ingredient_classification` reference fact to produce a `classified_ingredient`; then `calculate_ingredient_scaling_multiplier` combines the classification with a `ingredient_classification_scale_factor` reference fact; then `scale_ingredient_amount` computes the scaled quantity; then `convert_scaled_ingredient_to_optimal_measurement_unit` breaks it into human-readable units. Every fact in this chain has a clean, self-contained derivation. If you ask "why is flour scaled to 140 cups?" you get a readable tree that traces all the way back to the recipe input and the KB reference facts for flour's classification and scale factor.

Where the explanation component falls apart is in the [Planning Scaled Recipe Steps](/DOCS/planning/2.Planning.md) phase, for two reasons:

1. **Mutable state is invisible to the derivation tree.** The planning engine mutates equipment facts in place through `action_fn` callbacks. The derivation dict captures antecedent facts _at the time the rule fired_, but if those facts get mutated or removed later, the derivation tree is pointing at stale snapshots. You end up inspecting the derivation of a late-stage fact and seeing equipment states that no longer reflect what actually caused the rule to fire.

2. **Side-effect facts all share the same derivation context.** When an `action_fn` calls `wm.add_fact()` to assert side-effect facts, those facts inherit `wm._current_derivation`, which belongs to the rule's _consequent_, not to the individual side-effect. Every side-effect fact from the same rule firing ends up with the same derivation dict. Technically accurate (they were all produced by the same rule), but it tells you nothing about _why_ each specific side-effect was produced.

Two design changes would go a long way toward fixing this:

1. **Snapshot antecedent state.** Instead of storing references to live `Fact` objects in `derivation['antecedent_facts']`, store deep copies at the time the rule fires. That way the derivation tree preserves the actual state that triggered the rule, even if the original facts get mutated or retracted later. Facts are small data containers, so the memory cost is negligible.

2. **Track mutations explicitly.** Add a `mutation_log` to each fact: a list of `(rule_name, attribute, old_value, new_value)` entries that gets appended to every time an `action_fn` modifies the fact. The explanation facility could then show not just "this fact was derived by rule X" but also "this fact was subsequently modified by rules Y and Z." That would make the full equipment lifecycle (AVAILABLE to RESERVED to IN_USE to DIRTY to AVAILABLE) traceable instead of only showing the final state.

On top of the design changes, two kinds of additional knowledge would make a real difference:

1. **Natural-language rule descriptions.** Each `Rule` could carry a `description` field with a plain-English explanation of what it does ("Classifies an ingredient by looking it up in the known ingredient classifications table from the Knowledge Base"). Right now the explanation facility just prints the rule name (`derived by rule: 'classify_known_ingredient'`), which only makes sense if you already know the naming conventions. With a description field it could instead say something like: "This fact was derived because the system looked up FLOUR in the ingredient classification table and found it classified as DEFAULT." It requires writing a description for each rule, but the effort is minimal and the readability improvement is significant.

2. **Domain-specific formatting.** The explanation facility currently renders every fact the same way using `__str__`. If it understood the domain (for example, that a `scaled_ingredient` fact represents a quantity after applying a scaling factor), it could produce actual summaries instead of raw attribute dumps. Instead of `Fact #12: scaled_ingredient (ingredient_name=FLOUR, original_amount=2.25, scaled_amount=315.0, unit=CUPS)`, it could just say "FLOUR was scaled from 2.25 cups to 315.0 cups." You would need a registry of per-fact-title formatting functions or templates, basically a presentation layer that turns fact structures into natural language.
