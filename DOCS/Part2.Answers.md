# Part2 Assignment Answers

## 1.

> How well did you choose which hw1 rule set to use for this assignment, and why?  Did you encounter problems using it, and if so, what did you need to adapt?   What lessons did you learn from making the rules work in the application? 

As the submission evolved, the rules that were selected naturally evolved with the approach. The scaling rules from hw1 translated most directly. The full scaling rule chain was implemented: `classify_known_ingredient` matches a recipe ingredient against reference classification facts, falling back to `classify_default_ingredient` ingredients not in the knowledge base. Then `calculate_ingredient_scaling_multiplier` derives a per-ingredient multiplier by combining the target recipe scale factor with the classification-specific scale factor (e.g., baking soda → LEAVENING_AGENT → 70%, vanilla extract → EXTRACT → 60%, salt → SEASONING → 80%, flour → DEFAULT → 100%). Finally, `scale_ingredient_amount` computes the scaled quantity and `convert_scaled_ingredient_to_optimal_measurement_unit` breaks it into human-readable units, respecting heuristic measurements like PINCH and DASH when a recipe is scaled down and amounts become physically infeasible.

Rules associated with ingredient and equipment substitution were not used, as the submission did not advance to the point where they became relevant. We assumed the user had unlimited ingredients, and where equipment constraints were encountered we _attempted_ to apply batching rather than substitution.

The original hw1 batching rules were not used directly because they were too simple. They could identify that batching was required, but they had no understanding of where they were contextually in the overall recipe and we had no rules about what to actually DO once the consequent of `requires_batching` was added to Working Memory. In the end, I _attempted_ to implement batching from scratch using multi-phase dispatch. For example, the transfer dispatch rules plan how many dough balls fit per baking sheet and assert one `pending_transfer` fact per batch, then iteratively allocate sheets and execute transfers. The cook dispatch rules similarly plan how many sheets fit per oven rack, distribute across multiple ovens, and coordinate per-oven preheat substeps.

Several key lessons emerged from making these rules work in a real application:

1. **Context is everything.** The hw1 rules operated on isolated facts, but in an actual planning system every rule needs awareness of the current step, what equipment is in use, what capacity remains, and how many batches have already been allocated. This led to multi-antecedent rules with 4-5 conditions plus NegatedFact guards for iteration control.

2. **Pure rules are not enough for computation.** Complex calculations like how many dough balls fit on a sheet, how many sheets fit in an oven, how to break a scaled measurement into clean units, were implemented as action functions (`action_fn`) attached to rules rather than as additional rule chains. The rule-based dispatch still controls _when_ and _in what order_ these calculations fire, but the actual math lives in Python functions. This is a pragmatic hybrid: the rule engine handles pattern matching, conflict resolution, and chaining, while imperative code handles calculations and data structure manipulation.

3. **Equipment state management required a state machine.** A simple "is equipment available?" check from hw1 became a full lifecycle: AVAILABLE → RESERVED → IN_USE → DIRTY → (cleaning rule) → AVAILABLE. Surface equipment like cooling racks needed special treatment as always-available with infinite capacity. Tracking equipment contents via `equipment_contents` facts was essential for capacity-aware batching.

4. **Iteration in a forward-chaining system is unnatural.** Processing a list of items one-by-one required a NegatedFact-based iteration pattern: a rule fires once per pending item, consuming it via a NOT guard keyed on Working Memory size. Getting this right, especially preventing infinite loops while still allowing re-firing when new facts appear, was one of the most challenging adaptations.

## 2.

> In addition to your existing system, how would you design a question-asking component which asks for more information regarding the truth of certain assertions in the WM which the systems could not confirm? What heuristics could you use to ensure system efficiency? (guiding the system to ask more selective questions rather than asking users an exhaustive list of questions)

A question-asking component could be a complement to the existing `ExplanationFacility`, operating _during_ inference rather than after it. Where the explanation facility answers "why did the system believe X?", the question-asking component would address "should the system believe X at all?"

**Core Design.** Each `Fact` in Working Memory would gain an optional `confidence` attribute (a float from 0.0 to 1.0). Input facts provided directly by the user like `recipe_ingredient` facts parsed from the recipe file, and reference facts from the Knowledge Base would receive a confidence of 1.0, since these are given as ground truth. Derived facts would inherit confidence from their antecedents: specifically, the minimum confidence across all antecedent facts, optionally reduced by a per-rule reliability weight. For example, if `classify_known_ingredient` matches a recipe ingredient against a known KB classification, the derived `classified_ingredient` fact inherits full confidence. But if the fallback rule `classify_default_ingredient` fires instead (because no reference fact matched), the derived fact would receive a lower confidence to signal that the system is guessing.

When inference completes (or at configurable checkpoints mid-inference), the question-asking component would scan Working Memory for facts whose confidence falls below a configurable threshold. For each such fact, it would use the derivation chain (the same `fact.derivation` dict that the `ExplanationFacility` already walks) to generate a targeted question. For instance: "The system classified CREAM OF TARTAR as DEFAULT (100% scaling). Is this ingredient actually a leavening agent, a stabilizer, or something else?" The user's answer would be asserted as a new reference-level fact, the low-confidence fact would be retracted, and the engine would re-derive from that point forward with the corrected knowledge.

**Heuristics for Selective Questioning.** Naively asking about every uncertain fact would produce an exhausting list of questions. Several heuristics would keep the interaction efficient:

1. **Only question fallback-derived facts.** The scaling engine already has a natural signal for uncertainty: when `classify_default_ingredient` fires instead of `classify_known_ingredient`, that means the Knowledge Base lacked a specific classification. Rather than attaching confidence scores to every fact, the system could simply flag facts derived by designated "fallback" rules. This is the simplest and most targeted heuristic, since fallback rules exist precisely for cases where the KB is incomplete.

2. **Prioritize facts with high downstream impact.** A fact at the root of a long derivation chain affects many downstream conclusions. If `classified_ingredient` is wrong, then `ingredient_scaling_multiplier`, `scaled_ingredient`, and `optimally_scaled_ingredient` are all wrong. The system should walk the derivation graph forward (or maintain a reverse index of "which facts depend on this one") and prioritize questioning facts with the most dependents. Confirming or correcting one high-impact fact eliminates the need to question its entire downstream chain.

3. **Deduplicate by rule pattern.** If the same fallback rule fires for multiple ingredients (e.g., `classify_default_ingredient` fires for both CREAM OF TARTAR and CORN SYRUP), the system could batch these into a single interaction ("The following ingredients were not found in the knowledge base: ...") rather than asking one question per ingredient. This groups questions by the _rule_ that triggered uncertainty rather than by the _fact_ that was produced.

Together these three heuristics reduce questions from "one per fact in WM" to "one per gap in the Knowledge Base," which is exactly the right granularity: the system asks for help only when it genuinely lacks knowledge, and it asks efficiently by grouping related gaps together.

## 3. 

> Discuss the strengths and limitations of the explanation component.  Under what conditions would its explanations be likely to be useful?   Under what conditions would they be likely to be less useful?   How could you change the design to help alleviate these issues?   How could additional knowledge be applied to help, and what knowledge would be needed? 

**Strengths.** The explanation component's core strength is its recursive derivation tree. For any fact in Working Memory, a user can ask "why?" and receive a complete trace: the rule that derived it, the antecedent facts that triggered that rule, and recursively, the rules and antecedents behind each of those. Leaf facts are classified as either INPUT (asserted directly from the recipe) or REFERENCE (matched from the Knowledge Base), giving the user a clear sense of what was given versus what was looked up.

**When explanations are useful.** The explanation component works very well for linear, additive derivation chains where each rule produces a new fact from its antecedents without modifying existing state. This is exactly the pattern in the [Scaling Ingredients](/DOCS/scaling/1.Scaling.md) phase: `classify_known_ingredient` takes a `recipe_ingredient` and a `ingredient_classification` reference fact to produce a `classified_ingredient`; then `calculate_ingredient_scaling_multiplier` combines the classification with a `ingredient_classification_scale_factor` reference fact; then `scale_ingredient_amount` computes the scaled quantity; then `convert_scaled_ingredient_to_optimal_measurement_unit` breaks it into human-readable units. Each fact in this chain has a clean, self-contained derivation. Asking "why is flour scaled to 140 cups?" produces a readable tree that terminates at the recipe input and the KB reference facts for flour's classification and scale factor.

**When explanations break down.** The explanation component becomes much less useful in the [Planning Scaled Recipe Steps](/DOCS/planning/2.Planning.md) phase, for two reasons:

1. **Mutable state is invisible to the derivation tree.** The planning engine mutates equipment facts in place through the `action_fn` callbacks. The derivation dict records the antecedent facts _at the time the rule fired_, but if those facts are later mutated or removed, the derivation tree points to stale snapshots. A user inspecting the derivation of a late-stage fact may see antecedent equipment facts whose `state` attribute no longer reflects the state that actually caused the rule to fire.

2. **Side-effect facts share a single derivation context.** When an `action_fn` calls `wm.add_fact()` to assert side-effect facts, those facts inherit `wm._current_derivation`, which is the derivation of the rule's _consequent_, not a derivation specific to the side-effect. Multiple side-effect facts from the same rule firing all share the same derivation dict. This is technically accurate (they were all produced by the same rule) but provides no insight into _why_ each individual side-effect was produced.

**Design improvements.** Several changes would make the explanation component more useful in dynamic environments:

1. **Snapshot antecedent state.** Instead of storing references to live `Fact` objects in `derivation['antecedent_facts']`, store deep copies of those facts at the time the rule fires. This way the derivation tree preserves the actual state that triggered the rule, even if the original facts are later mutated or retracted. The cost is additional memory, but facts are small data containers, so the overhead would be minimal.

2. **Track mutations explicitly.** Introduce a `mutation_log` on each fact: a list of `(rule_name, attribute, old_value, new_value)` entries appended each time an `action_fn` modifies the fact in place. The explanation facility could then show not just "this fact was derived by rule X" but also "this fact was subsequently modified by rules Y and Z." This would make the equipment state machine (AVAILABLE to RESERVED to IN_USE to DIRTY to AVAILABLE) fully traceable rather than only showing the final state.

**Additional knowledge.** Two categories of additional knowledge would significantly improve explanations:

1. **Natural-language rule descriptions.** Each `Rule` could carry a `description` field with a human-readable explanation of its purpose ("Classifies an ingredient by looking it up in the known ingredient classifications table from the Knowledge Base"). The explanation facility currently prints the rule name (`derived by rule: 'classify_known_ingredient'`), which is interpretable only if the user understands the naming conventions. A description field would let the facility print something like: "This fact was derived because the system looked up FLOUR in the ingredient classification table and found it classified as DEFAULT." This requires domain-authored metadata on each rule, but the cost is low and the payoff for interpretability is substantial.

2. **Domain-specific formatting knowledge.** The explanation facility currently renders all facts identically using their `__str__` representation. With knowledge of the domain like "a `scaled_ingredient` fact represents a quantity of an ingredient after applying a scaling factor", the facility could produce contextual summaries instead of raw attribute dumps. For example, rather than printing `Fact #12: scaled_ingredient (ingredient_name=FLOUR, original_amount=2.25, scaled_amount=315.0, unit=CUPS)`, it could print "FLOUR was scaled from 2.25 cups to 315.0 cups." This would require a registry of per-fact-title formatting functions or templates, essentially a presentation layer that maps fact structures to natural language.
